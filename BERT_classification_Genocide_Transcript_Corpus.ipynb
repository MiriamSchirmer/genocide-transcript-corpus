{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf5I-AC2CAie"
   },
   "source": [
    "# ***Bert Text Classification***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials provided by Chris McCormick (https://mccormickml.com/) were used to create the basis of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "There are 2 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup, BertTokenizer\n",
    "from sklearn.metrics import roc_auc_score, classification_report, f1_score, accuracy_score, recall_score, precision_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "     print('There are %d GPU(s) available.' % torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "data = pd.read_csv(\"genocide-transcript-corpus-v0.1.csv\", sep=\";\")\n",
    "\n",
    "X_data = data['paragraph'].to_numpy()\n",
    "Y_data = data['label'].to_numpy()\n",
    "tribunals = data['tribunal'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for text tokenization\n",
    "\n",
    "def tokenize_data(text_samples):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    input_ids = []\n",
    "\n",
    "    print('Tokenizing paragraphs...')\n",
    "    \n",
    "    for sentence in text_samples:\n",
    "        encoded_par = tokenizer.encode(\n",
    "                            sentence,\n",
    "                            add_special_tokens = True,\n",
    "                       )\n",
    "\n",
    "        input_ids.append(encoded_par)\n",
    "\n",
    "    print(\"Number of samples:\", len(input_ids))\n",
    "    \n",
    "    input_ids = pad_sequences(input_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "# function for attention mask alignment\n",
    "\n",
    "def generate_attention_mask(tokenized_samples):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sample in tokenized_samples:\n",
    "        att_mask = [int(token_id > 0) for token_id in sample]\n",
    "        attention_masks.append(att_mask)\n",
    "        \n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates cross validation sets\n",
    "\n",
    "def split_data(x_data, y_data):\n",
    "    cv_folds = []\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(x_data):\n",
    "        X_train, X_test_eval = x_data[train_index], x_data[test_index]\n",
    "        y_train, y_test_eval = y_data[train_index], y_data[test_index]\n",
    "        \n",
    "        X_eval, X_test, y_eval, y_test = train_test_split(X_test_eval, y_test_eval, test_size=0.5, random_state=42)\n",
    "        \n",
    "        cv_folds.append((X_train, X_eval, X_test, y_train,\n",
    "                         y_eval, y_test))\n",
    "        print(\"X_train:\", cv_folds[-1][0].shape, \"X_eval:\", cv_folds[-1][1].shape, \"X_test:\", cv_folds[-1][2].shape,\n",
    "              \"y_train:\", cv_folds[-1][3].shape, \"y_eval:\", cv_folds[-1][4].shape, \"y_test:\", cv_folds[-1][5].shape,)\n",
    "        \n",
    "    return cv_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions\n",
    "# accuracy\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# duration\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# loss plot\n",
    "\n",
    "def plot_loss(loss_val):\n",
    "    sns.set(style='darkgrid')\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    plt.plot(loss_val, 'b-o')\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "# evaluation\n",
    "\n",
    "def evaluate(true, pred):\n",
    "    print('EVAL')\n",
    "    print(pred)\n",
    "    p1 = pred[:,1]\n",
    "    auc = roc_auc_score(true, p1)\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    \n",
    "    p1[p1 > 0] = 1\n",
    "    p1[p1 < 0] = 0\n",
    "    \n",
    "    print(classification_report(true, p1, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for model training\n",
    "\n",
    "def train_model(cross_fold_set, class_weights=None, batch_size=8, epochs=3):\n",
    "    X_train = cross_fold_set[0]\n",
    "    X_eval = cross_fold_set[1]\n",
    "    X_test = cross_fold_set[2]\n",
    "    y_train = cross_fold_set[3]\n",
    "    y_eval = cross_fold_set[4]\n",
    "    y_test = cross_fold_set[5]\n",
    "    \n",
    "    with open('logging.txt', 'a') as log_file:\n",
    "        log_file.write(str(X_train.shape[0]) + ',' + str(X_eval.shape[0]) + ',' + str(X_test.shape[0]) + ',')\n",
    "\n",
    "    # DataLoader for training set\n",
    "    train_inputs = tokenize_data(X_train)\n",
    "    train_attention_masks = generate_attention_mask(train_inputs)\n",
    "    train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_attention_masks), torch.tensor(y_train))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # DataLoader for validation set\n",
    "    eval_inputs = tokenize_data(X_eval)\n",
    "    eval_attention_masks = generate_attention_mask(eval_inputs)\n",
    "    validation_data = TensorDataset(torch.tensor(eval_inputs), torch.tensor(eval_attention_masks), torch.tensor(y_eval))\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False,\n",
    "    )\n",
    "    #model= torch.nn.DataParallel(model, device_ids = [0,2,3])\n",
    "    model= torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                          lr = 2e-5, \n",
    "                          eps = 1e-8\n",
    "                        )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = total_steps)\n",
    "    \n",
    "    loss_values = []\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        nb_train_steps = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 100 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "                \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()        \n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "            #print(\"LOSS\")\n",
    "            #print(F.cross_entropy(result.logits, b_labels))\n",
    "            \n",
    "            loss = F.cross_entropy(result.logits, b_labels, weight=class_weights.to(device))\n",
    "            #loss = result.loss.sum()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            train_logits = result.logits\n",
    "            train_logits = train_logits.detach().cpu().numpy()\n",
    "            train_label_ids = b_labels.to('cpu').numpy()\n",
    "            total_acc += flat_accuracy(train_logits, train_label_ids)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            nb_train_steps += 1\n",
    "            \n",
    "        avg_train_acc = total_acc / (nb_train_steps)\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        loss_values.append(avg_train_loss)\n",
    "        \n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Average training accuracy: {0:.2f}\".format(avg_train_acc))\n",
    "        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            #batch = tuple(t for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                result = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels,\n",
    "                               return_dict=True)\n",
    "\n",
    "            loss = result.loss.sum()\n",
    "            logits = result.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    print(\"Training complete!\")\n",
    "    #plot_loss(loss_values)\n",
    "    \n",
    "    print(\"Start Testing:\")\n",
    "    test_inputs = tokenize_data(X_test)\n",
    "    test_attention_masks = generate_attention_mask(test_inputs)\n",
    "    test_data = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_attention_masks), torch.tensor(y_test))\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions , true_labels = [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for (step, batch) in enumerate(test_dataloader):\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        batch = tuple(t for t in batch)\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           return_dict=True)\n",
    "        logits = result.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    print('DONE')\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    predictions = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    \n",
    "    print('EVAL')\n",
    "    print(predictions)\n",
    "    \n",
    "    print(classification_report(true_labels, predictions, digits=4))\n",
    "    \n",
    "    #evaluate(true_labels, predictions)\n",
    "    with open('logging.txt', 'a') as log_file:\n",
    "        log_file.write(str(accuracy_score(true_labels, predictions)) + ','\n",
    "                       + str(f1_score(true_labels, predictions, average='macro')) + ',' \n",
    "                       + str(f1_score(true_labels, predictions, average='micro')) + ','\n",
    "                       + str(f1_score(true_labels, predictions, average='binary')) + ','\n",
    "                       + str(precision_score(true_labels, predictions, average='macro')) + ','\n",
    "                       + str(precision_score(true_labels, predictions, average='micro')) + ','\n",
    "                       + str(precision_score(true_labels, predictions, average='binary')) + ','\n",
    "                       + str(recall_score(true_labels, predictions, average='macro')) + ','\n",
    "                       + str(recall_score(true_labels, predictions, average='micro'))+ ','\n",
    "                       + str(recall_score(true_labels, predictions, average='binary')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_by_id(tribunal_id):\n",
    "    indices = np.where(tribunals == tribunal_id)\n",
    "    return (np.take(X_data, indices)[0], np.take(Y_data, indices)[0])\n",
    "\n",
    "def get_samples_by_negative_id(not_tribunal_id):\n",
    "    indices = np.where(tribunals != not_tribunal_id)\n",
    "    return (np.take(X_data, indices)[0], np.take(Y_data, indices)[0])\n",
    "\n",
    "def get_train_eval(train_eval_tuple, test_tuple):\n",
    "    cv_folds = []\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(train_eval_tuple[0]):\n",
    "        X_train, X_eval = train_eval_tuple[0][train_index], train_eval_tuple[0][test_index]\n",
    "        y_train, y_eval = train_eval_tuple[1][train_index], train_eval_tuple[1][test_index]\n",
    "        cv_folds.append((X_train, X_eval, test_tuple[0], y_train, y_eval, test_tuple[1]))\n",
    "        print(\"X_train:\", cv_folds[-1][0].shape, \"X_eval:\", cv_folds[-1][1].shape, \"X_test:\", cv_folds[-1][2].shape,\n",
    "              \"y_train:\", cv_folds[-1][3].shape, \"y_eval:\", cv_folds[-1][4].shape, \"y_test:\", cv_folds[-1][5].shape,)\n",
    "    return cv_folds\n",
    "\n",
    "def split_by_sets(train_eval, test):\n",
    "    if train_eval == 0 and test == 0:\n",
    "        cv_splits = split_data(X_data, Y_data)\n",
    "    elif train_eval == 0:\n",
    "        cv_splits = get_train_eval(get_samples_by_negative_id(test), get_samples_by_id(test))\n",
    "    elif test == 0:\n",
    "        cv_splits = get_train_eval(get_samples_by_id(train_eval), get_samples_by_negative_id(train_eval))\n",
    "    elif train_eval == test:\n",
    "        cv_splits = split_data(get_samples_by_id(test)[0], get_samples_by_id(test)[1])\n",
    "    else:\n",
    "        cv_splits = get_train_eval(get_samples_by_id(train_eval), get_samples_by_id(test))\n",
    "    return cv_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(Y_data):\n",
    "    class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(Y_data), y=Y_data)\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(train_eval, test):\n",
    "    values = [0, 1, 2, 3]\n",
    "    if train_eval not in values or test not in values:\n",
    "        sys.exit(\"Falscher Parameter bei train_eval oder test\")\n",
    "    else:\n",
    "        cv_splits = split_by_sets(train_eval, test)\n",
    "    for idx, split in enumerate(cv_splits):\n",
    "        print(60*'*')\n",
    "        print(\"Using Split\", idx)\n",
    "        print(60*'*')\n",
    "        with open('logging.txt', 'a') as log_file:\n",
    "            log_file.write('\\n' + str(train_eval) + ',' + str(train_eval) + ',' + str(test) + ',' + str(idx) + ',')\n",
    "        train_model(split, class_weights=torch.tensor(compute_class_weights(split[3]), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (384,) X_eval: (96,) X_test: (995,) y_train: (384,) y_eval: (96,) y_test: (995,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (995,) y_train: (384,) y_eval: (96,) y_test: (995,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (995,) y_train: (384,) y_eval: (96,) y_test: (995,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (995,) y_train: (384,) y_eval: (96,) y_test: (995,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (995,) y_train: (384,) y_eval: (96,) y_test: (995,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.58\n",
      "  Average training accuracy: 0.73\n",
      "  Training epoch took: 0:00:17\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.90\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.42\n",
      "  Average training accuracy: 0.80\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.29\n",
      "  Average training accuracy: 0.89\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 995\n",
      "  Batch   100  of    125.    Elapsed: 0:00:11.\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8099    0.8311    0.8204       687\n",
      "           1     0.6000    0.5649    0.5819       308\n",
      "\n",
      "    accuracy                         0.7487       995\n",
      "   macro avg     0.7050    0.6980    0.7012       995\n",
      "weighted avg     0.7449    0.7487    0.7466       995\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.66\n",
      "  Average training accuracy: 0.63\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.42\n",
      "  Average training accuracy: 0.86\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.86\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.28\n",
      "  Average training accuracy: 0.90\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 995\n",
      "  Batch   100  of    125.    Elapsed: 0:00:11.\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
      " 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8810    0.8297    0.8546       687\n",
      "           1     0.6638    0.7500    0.7043       308\n",
      "\n",
      "    accuracy                         0.8050       995\n",
      "   macro avg     0.7724    0.7898    0.7794       995\n",
      "weighted avg     0.8138    0.8050    0.8080       995\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training accuracy: 0.68\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.75\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.35\n",
      "  Average training accuracy: 0.87\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.75\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.22\n",
      "  Average training accuracy: 0.91\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.76\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 995\n",
      "  Batch   100  of    125.    Elapsed: 0:00:11.\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1\n",
      " 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8244    0.8748    0.8489       687\n",
      "           1     0.6767    0.5844    0.6272       308\n",
      "\n",
      "    accuracy                         0.7849       995\n",
      "   macro avg     0.7506    0.7296    0.7380       995\n",
      "weighted avg     0.7787    0.7849    0.7802       995\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.58\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.42\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.73\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training accuracy: 0.89\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.77\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 995\n",
      "  Batch   100  of    125.    Elapsed: 0:00:11.\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
      " 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
      " 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      " 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8229    0.8253    0.8241       687\n",
      "           1     0.6078    0.6039    0.6059       308\n",
      "\n",
      "    accuracy                         0.7568       995\n",
      "   macro avg     0.7154    0.7146    0.7150       995\n",
      "weighted avg     0.7564    0.7568    0.7566       995\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training accuracy: 0.67\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.39\n",
      "  Average training accuracy: 0.83\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.30\n",
      "  Average training accuracy: 0.88\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.86\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 995\n",
      "  Batch   100  of    125.    Elapsed: 0:00:11.\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1\n",
      " 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
      " 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8551    0.8675    0.8613       687\n",
      "           1     0.6946    0.6721    0.6832       308\n",
      "\n",
      "    accuracy                         0.8070       995\n",
      "   macro avg     0.7749    0.7698    0.7722       995\n",
      "weighted avg     0.8054    0.8070    0.8061       995\n",
      "\n",
      "X_train: (384,) X_eval: (96,) X_test: (465,) y_train: (384,) y_eval: (96,) y_test: (465,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (465,) y_train: (384,) y_eval: (96,) y_test: (465,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (465,) y_train: (384,) y_eval: (96,) y_test: (465,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (465,) y_train: (384,) y_eval: (96,) y_test: (465,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (465,) y_train: (384,) y_eval: (96,) y_test: (465,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training accuracy: 0.69\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.42\n",
      "  Average training accuracy: 0.83\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.89\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.29\n",
      "  Average training accuracy: 0.89\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 465\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0\n",
      " 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7895    0.8916    0.8374       286\n",
      "           1     0.7817    0.6201    0.6916       179\n",
      "\n",
      "    accuracy                         0.7871       465\n",
      "   macro avg     0.7856    0.7559    0.7645       465\n",
      "weighted avg     0.7865    0.7871    0.7813       465\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.34\n",
      "  Average training accuracy: 0.88\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.23\n",
      "  Average training accuracy: 0.93\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.88\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 465\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
      " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8643    0.8462    0.8551       286\n",
      "           1     0.7622    0.7877    0.7747       179\n",
      "\n",
      "    accuracy                         0.8237       465\n",
      "   macro avg     0.8132    0.8169    0.8149       465\n",
      "weighted avg     0.8250    0.8237    0.8242       465\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.57\n",
      "  Average training accuracy: 0.76\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.37\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.73\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.28\n",
      "  Average training accuracy: 0.90\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.73\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 465\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
      " 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0\n",
      " 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1\n",
      " 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8635    0.7517    0.8037       286\n",
      "           1     0.6713    0.8101    0.7342       179\n",
      "\n",
      "    accuracy                         0.7742       465\n",
      "   macro avg     0.7674    0.7809    0.7690       465\n",
      "weighted avg     0.7895    0.7742    0.7770       465\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training accuracy: 0.68\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.77\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.40\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training accuracy: 0.88\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 465\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
      " 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8203    0.8776    0.8480       286\n",
      "           1     0.7799    0.6927    0.7337       179\n",
      "\n",
      "    accuracy                         0.8065       465\n",
      "   macro avg     0.8001    0.7852    0.7909       465\n",
      "weighted avg     0.8047    0.8065    0.8040       465\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.58\n",
      "  Average training accuracy: 0.71\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.38\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.26\n",
      "  Average training accuracy: 0.91\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 465\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7692    0.8741    0.8183       286\n",
      "           1     0.7429    0.5810    0.6520       179\n",
      "\n",
      "    accuracy                         0.7613       465\n",
      "   macro avg     0.7560    0.7276    0.7352       465\n",
      "weighted avg     0.7591    0.7613    0.7543       465\n",
      "\n",
      "X_train: (384,) X_eval: (96,) X_test: (530,) y_train: (384,) y_eval: (96,) y_test: (530,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (530,) y_train: (384,) y_eval: (96,) y_test: (530,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (530,) y_train: (384,) y_eval: (96,) y_test: (530,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (530,) y_train: (384,) y_eval: (96,) y_test: (530,)\n",
      "X_train: (384,) X_eval: (96,) X_test: (530,) y_train: (384,) y_eval: (96,) y_test: (530,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.45\n",
      "  Average training accuracy: 0.78\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.35\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.90\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 530\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0\n",
      " 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0\n",
      " 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 0 0 0 0 1 0 1 1 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8757    0.7556    0.8112       401\n",
      "           1     0.4674    0.6667    0.5495       129\n",
      "\n",
      "    accuracy                         0.7340       530\n",
      "   macro avg     0.6716    0.7111    0.6804       530\n",
      "weighted avg     0.7763    0.7340    0.7475       530\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.64\n",
      "  Average training accuracy: 0.62\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.39\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.88\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training accuracy: 0.91\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.86\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 530\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0\n",
      " 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
      " 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9441    0.7581    0.8409       401\n",
      "           1     0.5337    0.8605    0.6588       129\n",
      "\n",
      "    accuracy                         0.7830       530\n",
      "   macro avg     0.7389    0.8093    0.7498       530\n",
      "weighted avg     0.8442    0.7830    0.7966       530\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.59\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.36\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.24\n",
      "  Average training accuracy: 0.90\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.76\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 530\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0\n",
      " 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1\n",
      " 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 1 0 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9083    0.7656    0.8309       401\n",
      "           1     0.5104    0.7597    0.6106       129\n",
      "\n",
      "    accuracy                         0.7642       530\n",
      "   macro avg     0.7094    0.7626    0.7207       530\n",
      "weighted avg     0.8114    0.7642    0.7772       530\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.56\n",
      "  Average training accuracy: 0.73\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.74\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.39\n",
      "  Average training accuracy: 0.84\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.76\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.27\n",
      "  Average training accuracy: 0.92\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.78\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 530\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
      " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0\n",
      " 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1\n",
      " 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 1 0 1 1 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8882    0.7132    0.7911       401\n",
      "           1     0.4471    0.7209    0.5519       129\n",
      "\n",
      "    accuracy                         0.7151       530\n",
      "   macro avg     0.6677    0.7171    0.6715       530\n",
      "weighted avg     0.7808    0.7151    0.7329       530\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.57\n",
      "  Average training accuracy: 0.73\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.45\n",
      "  Average training accuracy: 0.80\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.73\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training accuracy: 0.88\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 530\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 1 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8790    0.7606    0.8155       401\n",
      "           1     0.4754    0.6744    0.5577       129\n",
      "\n",
      "    accuracy                         0.7396       530\n",
      "   macro avg     0.6772    0.7175    0.6866       530\n",
      "weighted avg     0.7807    0.7396    0.7528       530\n",
      "\n",
      "X_train: (384,) X_eval: (48,) X_test: (48,) y_train: (384,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (384,) X_eval: (48,) X_test: (48,) y_train: (384,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (384,) X_eval: (48,) X_test: (48,) y_train: (384,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (384,) X_eval: (48,) X_test: (48,) y_train: (384,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (384,) X_eval: (48,) X_test: (48,) y_train: (384,) y_eval: (48,) y_test: (48,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training accuracy: 0.64\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.39\n",
      "  Average training accuracy: 0.84\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.25\n",
      "  Average training accuracy: 0.92\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.81\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1\n",
      " 0 1 1 0 1 0 1 1 0 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.8824    0.8108        17\n",
      "           1     0.9286    0.8387    0.8814        31\n",
      "\n",
      "    accuracy                         0.8542        48\n",
      "   macro avg     0.8393    0.8605    0.8461        48\n",
      "weighted avg     0.8653    0.8542    0.8564        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training accuracy: 0.67\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.44\n",
      "  Average training accuracy: 0.81\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training accuracy: 0.88\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9091    0.6250    0.7407        16\n",
      "           1     0.8378    0.9688    0.8986        32\n",
      "\n",
      "    accuracy                         0.8542        48\n",
      "   macro avg     0.8735    0.7969    0.8196        48\n",
      "weighted avg     0.8616    0.8542    0.8459        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training accuracy: 0.67\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.65\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.43\n",
      "  Average training accuracy: 0.81\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.77\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.31\n",
      "  Average training accuracy: 0.89\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 0 1 1 0 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8824    0.7500    0.8108        20\n",
      "           1     0.8387    0.9286    0.8814        28\n",
      "\n",
      "    accuracy                         0.8542        48\n",
      "   macro avg     0.8605    0.8393    0.8461        48\n",
      "weighted avg     0.8569    0.8542    0.8520        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.69\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.37\n",
      "  Average training accuracy: 0.86\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.26\n",
      "  Average training accuracy: 0.91\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.81\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 1 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8780    0.9474    0.9114        38\n",
      "           1     0.7143    0.5000    0.5882        10\n",
      "\n",
      "    accuracy                         0.8542        48\n",
      "   macro avg     0.7962    0.7237    0.7498        48\n",
      "weighted avg     0.8439    0.8542    0.8441        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.65\n",
      "  Average training accuracy: 0.58\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.45\n",
      "  Average training accuracy: 0.82\n",
      "  Training epoch took: 0:00:16\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.32\n",
      "  Average training accuracy: 0.88\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8810    0.9487    0.9136        39\n",
      "           1     0.6667    0.4444    0.5333         9\n",
      "\n",
      "    accuracy                         0.8542        48\n",
      "   macro avg     0.7738    0.6966    0.7235        48\n",
      "weighted avg     0.8408    0.8542    0.8423        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters: \"train_eval\" und \"test\" with values 0-3 (0 -> \"complete dataset\"; 1-3 -> respective tribunal)\n",
    "\n",
    "for idx_1 in range(3, 4):\n",
    "    for idx_2 in range(0, 4):\n",
    "        combination(train_eval=idx_1, test=idx_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (274,) X_eval: (34,) X_test: (35,) y_train: (274,) y_eval: (34,) y_test: (35,)\n",
      "X_train: (274,) X_eval: (34,) X_test: (35,) y_train: (274,) y_eval: (34,) y_test: (35,)\n",
      "X_train: (274,) X_eval: (34,) X_test: (35,) y_train: (274,) y_eval: (34,) y_test: (35,)\n",
      "X_train: (275,) X_eval: (34,) X_test: (34,) y_train: (275,) y_eval: (34,) y_test: (34,)\n",
      "X_train: (275,) X_eval: (34,) X_test: (34,) y_train: (275,) y_eval: (34,) y_test: (34,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 274\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.75\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.67\n",
      "  Average training accuracy: 0.81\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.66\n",
      "  Average training accuracy: 0.82\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 35\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9143    1.0000    0.9552        32\n",
      "           1     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.9143        35\n",
      "   macro avg     0.4571    0.5000    0.4776        35\n",
      "weighted avg     0.8359    0.9143    0.8733        35\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing paragraphs...\n",
      "Number of samples: 274\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.68\n",
      "  Average training accuracy: 0.83\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.39\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.65\n",
      "  Average training accuracy: 0.82\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 35\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8824    0.9677    0.9231        31\n",
      "           1     0.0000    0.0000    0.0000         4\n",
      "\n",
      "    accuracy                         0.8571        35\n",
      "   macro avg     0.4412    0.4839    0.4615        35\n",
      "weighted avg     0.7815    0.8571    0.8176        35\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 274\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.74\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.88\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.68\n",
      "  Average training accuracy: 0.83\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.88\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.62\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.80\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 35\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9118    1.0000    0.9538        31\n",
      "           1     1.0000    0.2500    0.4000         4\n",
      "\n",
      "    accuracy                         0.9143        35\n",
      "   macro avg     0.9559    0.6250    0.6769        35\n",
      "weighted avg     0.9218    0.9143    0.8905        35\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 275\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.69\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.72\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.66\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.45\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7600    0.7308    0.7451        26\n",
      "           1     0.2222    0.2500    0.2353         8\n",
      "\n",
      "    accuracy                         0.6176        34\n",
      "   macro avg     0.4911    0.4904    0.4902        34\n",
      "weighted avg     0.6335    0.6176    0.6251        34\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 275\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.76\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.65\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.66\n",
      "  Average training accuracy: 0.83\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.65\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.67\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:11\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.65\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 34\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7647    1.0000    0.8667        26\n",
      "           1     0.0000    0.0000    0.0000         8\n",
      "\n",
      "    accuracy                         0.7647        34\n",
      "   macro avg     0.3824    0.5000    0.4333        34\n",
      "weighted avg     0.5848    0.7647    0.6627        34\n",
      "\n",
      "X_train: (384,) X_eval: (48,) X_test: (49,) y_train: (384,) y_eval: (48,) y_test: (49,)\n",
      "X_train: (385,) X_eval: (48,) X_test: (48,) y_train: (385,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (385,) X_eval: (48,) X_test: (48,) y_train: (385,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (385,) X_eval: (48,) X_test: (48,) y_train: (385,) y_eval: (48,) y_test: (48,)\n",
      "X_train: (385,) X_eval: (48,) X_test: (48,) y_train: (385,) y_eval: (48,) y_test: (48,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 384\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.72\n",
      "  Average training accuracy: 0.49\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.76\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.65\n",
      "  Average training accuracy: 0.84\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 49\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    1.0000    0.9231        42\n",
      "           1     0.0000    0.0000    0.0000         7\n",
      "\n",
      "    accuracy                         0.8571        49\n",
      "   macro avg     0.4286    0.5000    0.4615        49\n",
      "weighted avg     0.7347    0.8571    0.7912        49\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 385\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.72\n",
      "  Average training accuracy: 0.72\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.48\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.66\n",
      "  Average training accuracy: 0.63\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.63\n",
      "  Average training accuracy: 0.74\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.81\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1\n",
      " 0 0 1 1 0 0 1 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8649    0.7442    0.8000        43\n",
      "           1     0.0000    0.0000    0.0000         5\n",
      "\n",
      "    accuracy                         0.6667        48\n",
      "   macro avg     0.4324    0.3721    0.4000        48\n",
      "weighted avg     0.7748    0.6667    0.7167        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 385\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.72\n",
      "  Average training accuracy: 0.60\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.48\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.66\n",
      "  Average training accuracy: 0.78\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.79\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.61\n",
      "  Average training accuracy: 0.85\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.85\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7234    1.0000    0.8395        34\n",
      "           1     1.0000    0.0714    0.1333        14\n",
      "\n",
      "    accuracy                         0.7292        48\n",
      "   macro avg     0.8617    0.5357    0.4864        48\n",
      "weighted avg     0.8041    0.7292    0.6335        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 385\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.73\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.79\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.74\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.77\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.66\n",
      "  Average training accuracy: 0.79\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.75\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7805    0.8421    0.8101        38\n",
      "           1     0.1429    0.1000    0.1176        10\n",
      "\n",
      "    accuracy                         0.6875        48\n",
      "   macro avg     0.4617    0.4711    0.4639        48\n",
      "weighted avg     0.6476    0.6875    0.6659        48\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 385\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.60\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.77\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.67\n",
      "  Average training accuracy: 0.80\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:01\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.60\n",
      "  Average training accuracy: 0.84\n",
      "  Training epoch took: 0:00:15\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.88\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 48\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    1.0000    0.8571        36\n",
      "           1     0.0000    0.0000    0.0000        12\n",
      "\n",
      "    accuracy                         0.7500        48\n",
      "   macro avg     0.3750    0.5000    0.4286        48\n",
      "weighted avg     0.5625    0.7500    0.6429        48\n",
      "\n",
      "X_train: (248,) X_eval: (31,) X_test: (31,) y_train: (248,) y_eval: (31,) y_test: (31,)\n",
      "X_train: (248,) X_eval: (31,) X_test: (31,) y_train: (248,) y_eval: (31,) y_test: (31,)\n",
      "X_train: (248,) X_eval: (31,) X_test: (31,) y_train: (248,) y_eval: (31,) y_test: (31,)\n",
      "X_train: (248,) X_eval: (31,) X_test: (31,) y_train: (248,) y_eval: (31,) y_test: (31,)\n",
      "X_train: (248,) X_eval: (31,) X_test: (31,) y_train: (248,) y_eval: (31,) y_test: (31,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 248\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.76\n",
      "  Average training accuracy: 0.46\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.25\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.52\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.70\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.72\n",
      "  Validation took: 0:00:00\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9677    1.0000    0.9836        30\n",
      "           1     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.9677        31\n",
      "   macro avg     0.4839    0.5000    0.4918        31\n",
      "weighted avg     0.9365    0.9677    0.9519        31\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 248\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.71\n",
      "  Average training accuracy: 0.62\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.65\n",
      "  Average training accuracy: 0.83\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.45\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.65\n",
      "  Average training accuracy: 0.60\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.75\n",
      "  Validation took: 0:00:00\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.6923    0.7660        26\n",
      "           1     0.2000    0.4000    0.2667         5\n",
      "\n",
      "    accuracy                         0.6452        31\n",
      "   macro avg     0.5286    0.5462    0.5163        31\n",
      "weighted avg     0.7512    0.6452    0.6854        31\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 248\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.69\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.26\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.63\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.77\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.67\n",
      "  Average training accuracy: 0.74\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.71\n",
      "  Validation took: 0:00:00\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8966    0.9286    0.9123        28\n",
      "           1     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.8387        31\n",
      "   macro avg     0.4483    0.4643    0.4561        31\n",
      "weighted avg     0.8098    0.8387    0.8240        31\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 248\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.79\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.63\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.22\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.62\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7419    1.0000    0.8519        23\n",
      "           1     0.0000    0.0000    0.0000         8\n",
      "\n",
      "    accuracy                         0.7419        31\n",
      "   macro avg     0.3710    0.5000    0.4259        31\n",
      "weighted avg     0.5505    0.7419    0.6320        31\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 248\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Average training loss: 0.72\n",
      "  Average training accuracy: 0.77\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.97\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 3 ========\n",
      "  Average training loss: 0.68\n",
      "  Average training accuracy: 0.69\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.97\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 3 ========\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.50\n",
      "  Training epoch took: 0:00:10\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.58\n",
      "  Validation took: 0:00:00\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 31\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8095    0.6296    0.7083        27\n",
      "           1     0.0000    0.0000    0.0000         4\n",
      "\n",
      "    accuracy                         0.5484        31\n",
      "   macro avg     0.4048    0.3148    0.3542        31\n",
      "weighted avg     0.7051    0.5484    0.6169        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def compute_class_weights(Y_data):\n",
    "    class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(Y_data), y=Y_data)\n",
    "    return class_weight\n",
    "\n",
    "for tribunal_id in [1, 2, 3]:\n",
    "    X, Y = get_samples_by_id(tribunal_id)\n",
    "    idc_0 = np.where(Y==0)[0]\n",
    "    idc_1 = np.asarray(random.sample(list(np.where(Y==1)[0]), int(np.where(Y==0)[0].shape[0]*0.2)))\n",
    "    \n",
    "    X = np.concatenate((X[idc_0], X[idc_1]))\n",
    "    Y = np.concatenate((Y[idc_0], Y[idc_1]))\n",
    "\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(Y)\n",
    "\n",
    "    cv_splits = split_data(X, Y)\n",
    "    for idx, split in enumerate(cv_splits):\n",
    "        print(60*'*')\n",
    "        print(\"Using Split\", idx)\n",
    "        print(60*'*')\n",
    "        with open('logging.txt', 'a') as log_file:\n",
    "            log_file.write('\\n' + str(tribunal_id) + ',' + str(tribunal_id) + ',' + str(tribunal_id) + ',' + str(idx) + ',')\n",
    "        train_model(split, class_weights=torch.tensor(compute_class_weights(split[3]), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (907,) X_eval: (113,) X_test: (114,) y_train: (907,) y_eval: (113,) y_test: (114,)\n",
      "X_train: (907,) X_eval: (113,) X_test: (114,) y_train: (907,) y_eval: (113,) y_test: (114,)\n",
      "X_train: (907,) X_eval: (113,) X_test: (114,) y_train: (907,) y_eval: (113,) y_test: (114,)\n",
      "X_train: (907,) X_eval: (113,) X_test: (114,) y_train: (907,) y_eval: (113,) y_test: (114,)\n",
      "X_train: (908,) X_eval: (113,) X_test: (113,) y_train: (908,) y_eval: (113,) y_test: (113,)\n",
      "************************************************************\n",
      "Using Split 0\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 907\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.70\n",
      "  Average training accuracy: 0.67\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 2 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.68\n",
      "  Average training accuracy: 0.71\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 3 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.63\n",
      "  Average training accuracy: 0.72\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:02\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 114\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8288    0.9684    0.8932        95\n",
      "           1     0.0000    0.0000    0.0000        19\n",
      "\n",
      "    accuracy                         0.8070       114\n",
      "   macro avg     0.4144    0.4842    0.4466       114\n",
      "weighted avg     0.6907    0.8070    0.7443       114\n",
      "\n",
      "************************************************************\n",
      "Using Split 1\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 907\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.68\n",
      "  Average training accuracy: 0.74\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.87\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 2 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.72\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 3 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.67\n",
      "  Average training accuracy: 0.75\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:02\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 114\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8056    0.9457    0.8700        92\n",
      "           1     0.1667    0.0455    0.0714        22\n",
      "\n",
      "    accuracy                         0.7719       114\n",
      "   macro avg     0.4861    0.4956    0.4707       114\n",
      "weighted avg     0.6823    0.7719    0.7159       114\n",
      "\n",
      "************************************************************\n",
      "Using Split 2\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 907\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.72\n",
      "  Average training accuracy: 0.64\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.84\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 2 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.61\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.83\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 3 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.62\n",
      "  Average training accuracy: 0.75\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.68\n",
      "  Validation took: 0:00:02\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 114\n",
      "DONE\n",
      "EVAL\n",
      "[0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0\n",
      " 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.7660    0.8090        94\n",
      "           1     0.2667    0.4000    0.3200        20\n",
      "\n",
      "    accuracy                         0.7018       114\n",
      "   macro avg     0.5619    0.5830    0.5645       114\n",
      "weighted avg     0.7536    0.7018    0.7232       114\n",
      "\n",
      "************************************************************\n",
      "Using Split 3\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 907\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.74\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.87\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 2 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.67\n",
      "  Average training accuracy: 0.78\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.81\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 3 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.62\n",
      "  Average training accuracy: 0.80\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:02\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 114\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7619    0.8989    0.8247        89\n",
      "           1     0.0000    0.0000    0.0000        25\n",
      "\n",
      "    accuracy                         0.7018       114\n",
      "   macro avg     0.3810    0.4494    0.4124       114\n",
      "weighted avg     0.5948    0.7018    0.6439       114\n",
      "\n",
      "************************************************************\n",
      "Using Split 4\n",
      "************************************************************\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 908\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.71\n",
      "  Average training accuracy: 0.59\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 2 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.69\n",
      "  Average training accuracy: 0.76\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.50\n",
      "  Validation took: 0:00:02\n",
      "======== Epoch 3 / 3 ========\n",
      "  Batch   100  of    114.    Elapsed: 0:00:32.\n",
      "  Average training loss: 0.68\n",
      "  Average training accuracy: 0.78\n",
      "  Training epoch took: 0:00:36\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.82\n",
      "  Validation took: 0:00:02\n",
      "Training complete!\n",
      "Start Testing:\n",
      "Tokenizing paragraphs...\n",
      "Number of samples: 113\n",
      "DONE\n",
      "EVAL\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8496    1.0000    0.9187        96\n",
      "           1     0.0000    0.0000    0.0000        17\n",
      "\n",
      "    accuracy                         0.8496       113\n",
      "   macro avg     0.4248    0.5000    0.4593       113\n",
      "weighted avg     0.7217    0.8496    0.7805       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def compute_class_weights(Y_data):\n",
    "    class_weight = compute_class_weight(class_weight='balanced', classes=np.unique(Y_data), y=Y_data)\n",
    "    return class_weight\n",
    "\n",
    "X_all = []\n",
    "Y_all = []\n",
    "\n",
    "for tribunal_id in [1, 2, 3]:\n",
    "    X, Y = get_samples_by_id(tribunal_id)\n",
    "    idc_0 = np.where(Y==0)[0]\n",
    "    idc_1 = np.asarray(random.sample(list(np.where(Y==1)[0]), int(np.where(Y==0)[0].shape[0]*0.2)))\n",
    "    \n",
    "    X_all.append(np.concatenate((X[idc_0], X[idc_1])))\n",
    "    Y_all.append(np.concatenate((Y[idc_0], Y[idc_1])))\n",
    "    \n",
    "X = np.concatenate(X_all)\n",
    "Y = np.concatenate(Y_all)\n",
    "\n",
    "np.random.shuffle(X)\n",
    "np.random.shuffle(Y)\n",
    "\n",
    "cv_splits = split_data(X, Y)\n",
    "for idx, split in enumerate(cv_splits):\n",
    "    print(60*'*')\n",
    "    print(\"Using Split\", idx)\n",
    "    print(60*'*')\n",
    "    with open('logging.txt', 'a') as log_file:\n",
    "        log_file.write('\\n' + str(tribunal_id) + ',' + str(tribunal_id) + ',' + str(tribunal_id) + ',' + str(idx) + ',')\n",
    "    train_model(split, class_weights=torch.tensor(compute_class_weights(split[3]), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_text_classification_Cambodia.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0372e9731132487a82c4ba25ffc15291": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06b1d29257604730809611f767399fbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0997b623e71a470d9a68fb034ede2219": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d2f6de3a8b648d1a3091161081f7088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f93a61e87ac41668ee04be3ef847888",
      "placeholder": "​",
      "style": "IPY_MODEL_2ebdd0f1da5d4d6ab6f65c762d5459ed",
      "value": " 226k/226k [00:00&lt;00:00, 1.50MB/s]"
     }
    },
    "0f516bfe973d443095a128579b051254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fbaf2d40f644f009c62ff23906ab11c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10f4f604366d45caae924d1b8a45b7a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13586d40bcb54afa89a2a08df7e321e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13b136152fb5466cb05b710dee8c944a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31ed419cdd2445c8913b31b402e70f2c",
      "placeholder": "​",
      "style": "IPY_MODEL_da6c0e98c1a84586bfd8c9acce94499f",
      "value": " 420M/420M [00:15&lt;00:00, 29.0MB/s]"
     }
    },
    "1881449ea72d4b4d85c492ab4ca4d1b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ab72b1eff234eaba4611136d6d96395": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c29e4f744e349d5a864ec9a7ed48fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2100291574b84719b14f6906fe88e2eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ae643b265ff49e2b46bd7017177cd57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61a0571459834c8ebef6741b7f055192",
       "IPY_MODEL_981532fe55a94971a4408df06504f6bc",
       "IPY_MODEL_13b136152fb5466cb05b710dee8c944a"
      ],
      "layout": "IPY_MODEL_81f8e137591d4fcab791e69d7d478215"
     }
    },
    "2dccb29d95a9468f942927b7df00fb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13586d40bcb54afa89a2a08df7e321e4",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de09008423374983b7516d1956e1fa32",
      "value": 231508
     }
    },
    "2ebdd0f1da5d4d6ab6f65c762d5459ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ed419cdd2445c8913b31b402e70f2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32e7da5427504018b94a17073d7b2ca8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7be561927634711bbf21c41ea897a6d",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdfcaae624524290b521973ceb32ade4",
      "value": 570
     }
    },
    "417d4b9a67324ff294cecc5e4b88d528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1881449ea72d4b4d85c492ab4ca4d1b8",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db4043e6638f4709a82b7cf3f75fc101",
      "value": 28
     }
    },
    "443ce8a5cad4412ab005a3d2227a5892": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "480e2f6718c942c8b167116cb3e8c761": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52a6784b70df499fa17d7cfbe0b94de6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56af4edc55254ebfbb90f4842b35b52b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94127049263741278788217cbc2bb838",
      "placeholder": "​",
      "style": "IPY_MODEL_0fbaf2d40f644f009c62ff23906ab11c",
      "value": "Downloading: 100%"
     }
    },
    "580e3ef5de4c45788f8780c35b2a7d94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d2522d5b79d4d739c7ec03d9beaa9f5",
       "IPY_MODEL_417d4b9a67324ff294cecc5e4b88d528",
       "IPY_MODEL_9f6a21a5bca24d88aa518271eb89fe32"
      ],
      "layout": "IPY_MODEL_0372e9731132487a82c4ba25ffc15291"
     }
    },
    "58c3b130cc4844b09688f316519d8931": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f8c1a3c02224b6698005872e25145eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fdc101a084d4ef59d63e8204d1503be",
      "placeholder": "​",
      "style": "IPY_MODEL_52a6784b70df499fa17d7cfbe0b94de6",
      "value": "Downloading: 100%"
     }
    },
    "61a0571459834c8ebef6741b7f055192": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90306cb4605d45bb8e01ac73656957f9",
      "placeholder": "​",
      "style": "IPY_MODEL_892dede38434425993b7fc93544345ff",
      "value": "Downloading: 100%"
     }
    },
    "7b2ade2bf7fb4e3197ed597d2061a6fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cb0c46b19b24d9aa6fd3e6413d2b8c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56af4edc55254ebfbb90f4842b35b52b",
       "IPY_MODEL_32e7da5427504018b94a17073d7b2ca8",
       "IPY_MODEL_aea05988862c46059c6745d46743a89a"
      ],
      "layout": "IPY_MODEL_443ce8a5cad4412ab005a3d2227a5892"
     }
    },
    "81f8e137591d4fcab791e69d7d478215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "892dede38434425993b7fc93544345ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f93a61e87ac41668ee04be3ef847888": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90306cb4605d45bb8e01ac73656957f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94127049263741278788217cbc2bb838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96dc39bf7b7a4ac08c782f6903e43c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_480e2f6718c942c8b167116cb3e8c761",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f516bfe973d443095a128579b051254",
      "value": 466062
     }
    },
    "981532fe55a94971a4408df06504f6bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10f4f604366d45caae924d1b8a45b7a5",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bed98881118a44ec882eb9bfc0ac3cbb",
      "value": 440473133
     }
    },
    "9d2522d5b79d4d739c7ec03d9beaa9f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0997b623e71a470d9a68fb034ede2219",
      "placeholder": "​",
      "style": "IPY_MODEL_d85bd7637e2a4b5dab7fbdc142e21175",
      "value": "Downloading: 100%"
     }
    },
    "9f6a21a5bca24d88aa518271eb89fe32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06b1d29257604730809611f767399fbe",
      "placeholder": "​",
      "style": "IPY_MODEL_fef36c0d408c4d52a38894c3f60ba8f4",
      "value": " 28.0/28.0 [00:00&lt;00:00, 585B/s]"
     }
    },
    "9fdc101a084d4ef59d63e8204d1503be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7be561927634711bbf21c41ea897a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abf8817d38cb498c9f25a8d953b68bdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aea05988862c46059c6745d46743a89a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58c3b130cc4844b09688f316519d8931",
      "placeholder": "​",
      "style": "IPY_MODEL_abf8817d38cb498c9f25a8d953b68bdc",
      "value": " 570/570 [00:00&lt;00:00, 15.3kB/s]"
     }
    },
    "bdfcaae624524290b521973ceb32ade4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bed98881118a44ec882eb9bfc0ac3cbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c78d756b7bf845baa3bb9cbe2782718b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d2200047bbcb4fca821b4e906453c83e",
       "IPY_MODEL_2dccb29d95a9468f942927b7df00fb52",
       "IPY_MODEL_0d2f6de3a8b648d1a3091161081f7088"
      ],
      "layout": "IPY_MODEL_1ab72b1eff234eaba4611136d6d96395"
     }
    },
    "d2200047bbcb4fca821b4e906453c83e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daf01c54f7b14027af0b6b9dc931e7e3",
      "placeholder": "​",
      "style": "IPY_MODEL_1c29e4f744e349d5a864ec9a7ed48fc8",
      "value": "Downloading: 100%"
     }
    },
    "d85bd7637e2a4b5dab7fbdc142e21175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da6c0e98c1a84586bfd8c9acce94499f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "daf01c54f7b14027af0b6b9dc931e7e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db4043e6638f4709a82b7cf3f75fc101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de09008423374983b7516d1956e1fa32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1c373670f304a26ad8af4f29325d49d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b2ade2bf7fb4e3197ed597d2061a6fd",
      "placeholder": "​",
      "style": "IPY_MODEL_2100291574b84719b14f6906fe88e2eb",
      "value": " 455k/455k [00:00&lt;00:00, 1.35MB/s]"
     }
    },
    "e7ea323e843f403d94b3dba324d9db67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f8c1a3c02224b6698005872e25145eb",
       "IPY_MODEL_96dc39bf7b7a4ac08c782f6903e43c65",
       "IPY_MODEL_e1c373670f304a26ad8af4f29325d49d"
      ],
      "layout": "IPY_MODEL_eebe95615254468cbe856e253275678a"
     }
    },
    "eebe95615254468cbe856e253275678a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fef36c0d408c4d52a38894c3f60ba8f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
